{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de920aa7",
   "metadata": {},
   "source": [
    "##  Predict your coarse-grained genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76513f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from collections import Counter\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2891e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"../data/intermediate/df_phase1.csv\")\n",
    "df_classif = pd.read_csv(\"../data/intermediate/df_classif_phase1.csv\")\n",
    "df_audio = pd.read_csv(\"../data/intermediate/df_audio_phase1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d1714",
   "metadata": {},
   "source": [
    "## 1. Taxonomie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d1ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taxonomie (4 groupes) :\n",
      "              genre_top  cluster_id_4\n",
      "1             Classical             0\n",
      "5          Experimental             0\n",
      "10                 Jazz             0\n",
      "9         International             0\n",
      "12                  Pop             1\n",
      "11  Old-Time / Historic             1\n",
      "4            Electronic             1\n",
      "7               Hip-Hop             1\n",
      "15               Spoken             1\n",
      "2               Country             2\n",
      "0                 Blues             3\n",
      "3        Easy Listening             3\n",
      "6                  Folk             3\n",
      "8          Instrumental             3\n",
      "13                 Rock             3\n",
      "14             Soul-RnB             3\n"
     ]
    }
   ],
   "source": [
    "# 0. Préparation des données\n",
    "\n",
    "# On part de df_classif avec 'genre_top' et 'genres_all_names'\n",
    "\n",
    "# Conversion en liste Python\n",
    "if isinstance(df_classif[\"genres_all_names\"].iloc[0], str):\n",
    "    df_classif[\"genres_all_names\"] = df_classif[\"genres_all_names\"].apply(ast.literal_eval)\n",
    "\n",
    "df_g = df_classif.copy()\n",
    "\n",
    "\n",
    "# 1. Matrice genre_top x sous-genres\n",
    "\n",
    "# Lister les sous-genres les plus fréquents\n",
    "all_tags = [tag for sub in df_g[\"genres_all_names\"] for tag in sub]\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "# Garde les 100 sous-genres les plus fréquents\n",
    "top_tags = [t for t, c in tag_counts.most_common(100)]\n",
    "tag_to_idx = {t: i for i, t in enumerate(top_tags)}\n",
    "\n",
    "# Initialiser la matrice de co-occurrence\n",
    "genres = sorted(df_g[\"genre_top\"].unique())\n",
    "genre_to_idx = {g: i for i, g in enumerate(genres)}\n",
    "\n",
    "cooc = np.zeros((len(genres), len(top_tags)), dtype=float)\n",
    "\n",
    "# Remplir la matrice : co-occurrence genre_top / sous-genre\n",
    "for _, row in df_g[[\"genre_top\", \"genres_all_names\"]].iterrows():\n",
    "    gi = genre_to_idx[row[\"genre_top\"]]\n",
    "    for tag in row[\"genres_all_names\"]:\n",
    "        if tag in tag_to_idx:\n",
    "            ti = tag_to_idx[tag]\n",
    "            cooc[gi, ti] += 1\n",
    "\n",
    "# Normalisation ligne : chaque genre devient un \"profil\" de sous-genres\n",
    "cooc_norm = normalize(cooc, norm=\"l2\", axis=1)\n",
    "\n",
    "# 2. Réduction de dimension \n",
    "\n",
    "# SVD tronquée pour projeter dans un espace de petite dimension \n",
    "svd = TruncatedSVD(n_components=5, random_state=42)\n",
    "cooc_reduced = svd.fit_transform(cooc_norm)\n",
    "\n",
    "\n",
    "# 3. Clustering en 4 groupes coarse\n",
    "\n",
    "kmeans_4 = KMeans(n_clusters=4, random_state=42, n_init=\"auto\")\n",
    "clusters_4 = kmeans_4.fit_predict(cooc_reduced)\n",
    "\n",
    "taxo4 = pd.DataFrame({\n",
    "    \"genre_top\": genres,\n",
    "    \"cluster_id_4\": clusters_4\n",
    "}).sort_values(\"cluster_id_4\")\n",
    "\n",
    "print(\"\\nTaxonomie (4 groupes) :\")\n",
    "print(taxo4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e62181",
   "metadata": {},
   "source": [
    "- On obtient en 4 groupes.\n",
    "\n",
    "- Cluster 0 : Classical, Experimental, Jazz, International → famille “Art / Spécialisé”, des genres souvent plus instrumentaux, ou non‑mainstream.\n",
    "- Cluster 1 : Pop, Old-Time / Historic, Electronic, Hip-Hop, Spoken → famille “Mainstream / Moderne”, centrée sur les musiques populaires et les formes parlées.\n",
    "- Cluster 2 : Country isolé → profil de sous‑genres très spécifique (country/americana)\n",
    "- Cluster 3 : Blues, Easy Listening, Folk, Instrumental, Rock, Soul-RnB → gros bloc “Rock / Roots & Soul”, mêlant rock, blues, folk et variantes plus douces/instrumentales.\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8cc764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id_4\n",
      "3    16697\n",
      "1    15152\n",
      "0    12621\n",
      "2      163\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ajouter le cluster au df_classif via un merge\n",
    "cluster_map = dict(zip(genres, clusters_4))  \n",
    "df_classif[\"cluster_id_4\"] = df_classif[\"genre_top\"].map(cluster_map)\n",
    "\n",
    "# Compter le nombre de morceaux par cluster\n",
    "print(df_classif[\"cluster_id_4\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93cb90b",
   "metadata": {},
   "source": [
    "- Le cluster 2 ne contient que 163 morceaux, alors que les autres en ont entre 12 621 et 16 697 : c’est une classe ultra‑minoritaire par rapport au reste.\n",
    "- Avec si peu d’exemples, les modèles supervisés apprennent très mal ce groupe (rappel quasi nul), ce qui tire vers le bas la macro‑F1 même quand l’accuracy globale est bonne.\n",
    "- Comme Country est musicalement proche de Folk / Blues / Rock (présents dans le cluster 3 “roots/rock/soul”), le choix raisonnable est de fusionner le cluster 2 avec le cluster 3 pour obtenir trois familles plus équilibrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8035df03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre_coarse\n",
      "Rock/Roots/Country&Soul    16860\n",
      "Mainstream                 15152\n",
      "Art/Spécialisé             12621\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "coarse_map = {\n",
    "    # Groupe 1 : Art / Spécialisé (cluster 0)\n",
    "    \"Classical\": \"Art/Spécialisé\",\n",
    "    \"Experimental\": \"Art/Spécialisé\",\n",
    "    \"Jazz\": \"Art/Spécialisé\",\n",
    "    \"International\": \"Art/Spécialisé\",\n",
    "\n",
    "    # Groupe 2 : Mainstream (cluster 1)\n",
    "    \"Pop\": \"Mainstream\",\n",
    "    \"Electronic\": \"Mainstream\",\n",
    "    \"Hip-Hop\": \"Mainstream\",\n",
    "    \"Spoken\": \"Mainstream\",\n",
    "    \"Old-Time / Historic\": \"Mainstream\",\n",
    "\n",
    "    # Groupe 3 : Rock / Roots / Country & Soul (clusters 2 + 3)\n",
    "    \"Country\": \"Rock/Roots/Country&Soul\",\n",
    "    \"Blues\": \"Rock/Roots/Country&Soul\",\n",
    "    \"Easy Listening\": \"Rock/Roots/Country&Soul\",\n",
    "    \"Folk\": \"Rock/Roots/Country&Soul\",\n",
    "    \"Instrumental\": \"Rock/Roots/Country&Soul\",\n",
    "    \"Rock\": \"Rock/Roots/Country&Soul\",\n",
    "    \"Soul-RnB\": \"Rock/Roots/Country&Soul\",\n",
    "}\n",
    "\n",
    "df_classif[\"genre_coarse\"] = df_classif[\"genre_top\"].map(coarse_map)\n",
    "print(df_classif[\"genre_coarse\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeaaf97",
   "metadata": {},
   "source": [
    "## 2. Encodage de la cible et choix des features\n",
    "\n",
    "On encode la nouvelle cible `genre_coarse` en entiers pour les modèles de classification,\n",
    "et on sélectionne les mêmes features numériques que ceux choisi pour la task1 pour pouvoir comparer à la fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1107d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de features : 41\n",
      "Features utilisées : ['album_tracks', 'artist_latitude', 'artist_longitude', 'duration', 'favorites', 'interest', 'listens', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'speechiness', 'tempo', 'valence', 'spectral_bandwidth_kurtosis_01', 'spectral_bandwidth_max_01', 'spectral_bandwidth_mean_01', 'spectral_bandwidth_median_01', 'spectral_bandwidth_min_01', 'spectral_bandwidth_skew_01', 'spectral_bandwidth_std_01', 'spectral_centroid_kurtosis_01', 'spectral_centroid_max_01', 'spectral_centroid_mean_01', 'spectral_centroid_median_01', 'spectral_centroid_min_01', 'spectral_centroid_skew_01', 'spectral_centroid_std_01', 'spectral_rolloff_kurtosis_01', 'spectral_rolloff_max_01', 'spectral_rolloff_mean_01', 'spectral_rolloff_median_01', 'spectral_rolloff_min_01', 'spectral_rolloff_skew_01', 'spectral_rolloff_std_01', 'log_duration', 'log_listens', 'log_favorites', 'duration_min', 'favorites_per_listen']\n",
      "X shape : (44633, 41) | y shape : (44633,)\n"
     ]
    }
   ],
   "source": [
    "# Encoder la cible coarse (4 classes)\n",
    "le_coarse = LabelEncoder()\n",
    "df_classif[\"genre_coarse_encoded\"] = le_coarse.fit_transform(df_classif[\"genre_coarse\"])\n",
    "\n",
    "# Colonnes à exclure des features (cibles, id, listes de genres, texte)\n",
    "cols_to_drop = [\n",
    "    \"track_id\",\n",
    "    \"genre_top\",\n",
    "    \"genre_encoded\",\n",
    "    \"genre_coarse\",\n",
    "    \"genre_coarse_encoded\",\n",
    "    \"genres\",\n",
    "    \"genres_all\",\n",
    "    \"genres_names\",\n",
    "    \"genres_all_names\",\n",
    "    \"artist_name\",\n",
    "    \"album_title\",\n",
    "    \"title\",\n",
    "]\n",
    "\n",
    "cols_to_drop = [c for c in cols_to_drop if c in df_classif.columns]\n",
    "\n",
    "# On part de toutes les colonnes sauf celles à exclure\n",
    "X_full = df_classif.drop(columns=cols_to_drop)\n",
    "\n",
    "# On garde uniquement les colonnes numériques pour les modèles sklearn\n",
    "feature_cols = X_full.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "X = X_full[feature_cols].fillna(0)\n",
    "y = df_classif[\"genre_coarse_encoded\"]\n",
    "\n",
    "print(\"Nb de features :\", len(feature_cols))\n",
    "print(\"Features utilisées :\", feature_cols)\n",
    "print(\"X shape :\", X.shape, \"| y shape :\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38736bf5",
   "metadata": {},
   "source": [
    "## 3.Split train / validation / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19f1ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : (26779, 41)  Val : (8927, 41)  Test : (8927, 41)\n"
     ]
    }
   ],
   "source": [
    "# Train+Val vs Test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train vs Val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")  \n",
    "\n",
    "print(\"Train :\", X_train.shape, \" Val :\", X_val.shape, \" Test :\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764666d9",
   "metadata": {},
   "source": [
    "## 4. Entraînement de plusieurs modèles sur le set de validation\n",
    "\n",
    "On teste plusieurs familles de modèles de classification :\n",
    "- modèles linéaires (Logistic Regression),\n",
    "- modèles à base de distances (kNN, SVM),\n",
    "- modèles d'arbres (Decision Tree, Random Forest),\n",
    "- modèles de boosting (Gradient Boosting, AdaBoost),\n",
    "- gradient de boosting (XGBoost, LightGBM),\n",
    "- réseaux de neurones (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f4d9bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogReg (validation) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.621     0.521     0.566      2524\n",
      "           1      0.650     0.597     0.622      3031\n",
      "           2      0.599     0.715     0.652      3372\n",
      "\n",
      "    accuracy                          0.620      8927\n",
      "   macro avg      0.623     0.611     0.614      8927\n",
      "weighted avg      0.622     0.620     0.618      8927\n",
      "\n",
      "\n",
      "=== kNN (validation) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.706     0.565     0.628      2524\n",
      "           1      0.682     0.683     0.683      3031\n",
      "           2      0.653     0.750     0.698      3372\n",
      "\n",
      "    accuracy                          0.675      8927\n",
      "   macro avg      0.681     0.666     0.669      8927\n",
      "weighted avg      0.678     0.675     0.673      8927\n",
      "\n",
      "\n",
      "=== SVM_RBF (validation) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.699     0.653     0.675      2524\n",
      "           1      0.738     0.700     0.719      3031\n",
      "           2      0.695     0.762     0.727      3372\n",
      "\n",
      "    accuracy                          0.710      8927\n",
      "   macro avg      0.711     0.705     0.707      8927\n",
      "weighted avg      0.711     0.710     0.709      8927\n",
      "\n",
      "\n",
      "=== DecisionTree (validation) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.572     0.586     0.579      2524\n",
      "           1      0.608     0.599     0.603      3031\n",
      "           2      0.619     0.616     0.617      3372\n",
      "\n",
      "    accuracy                          0.602      8927\n",
      "   macro avg      0.600     0.600     0.600      8927\n",
      "weighted avg      0.602     0.602     0.602      8927\n",
      "\n",
      "\n",
      "=== RandomForest (validation) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.714     0.671     0.692      2524\n",
      "           1      0.731     0.710     0.720      3031\n",
      "           2      0.699     0.748     0.723      3372\n",
      "\n",
      "    accuracy                          0.713      8927\n",
      "   macro avg      0.715     0.710     0.712      8927\n",
      "weighted avg      0.714     0.713     0.713      8927\n",
      "\n",
      "\n",
      "=== GradBoost (validation) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.694     0.624     0.657      2524\n",
      "           1      0.719     0.690     0.704      3031\n",
      "           2      0.664     0.738     0.699      3372\n",
      "\n",
      "    accuracy                          0.689      8927\n",
      "   macro avg      0.692     0.684     0.687      8927\n",
      "weighted avg      0.691     0.689     0.689      8927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    # Modèles qui ont besoin de scaling\n",
    "    \"LogReg\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, n_jobs=-1))\n",
    "    ]),\n",
    "    \"kNN\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", KNeighborsClassifier(n_neighbors=15))\n",
    "    ]),\n",
    "    \"SVM_RBF\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", SVC(kernel=\"rbf\", C=3, gamma=\"scale\"))\n",
    "    ]),\n",
    "\n",
    "    # Modèles arbres / boosting (pas besoin de scaler)\n",
    "    \"DecisionTree\": DecisionTreeClassifier(max_depth=None, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=None, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"GradBoost\": GradientBoostingClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} (validation) ===\")\n",
    "    # Apprentissage sur le train\n",
    "    model.fit(X_train, y_train)\n",
    "    # Prédictions sur le set de validation\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    # Rapport de classification\n",
    "    print(classification_report(y_val, y_val_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213dce1",
   "metadata": {},
   "source": [
    "- Accuracy globale : de 0.60 (DecisionTree) à 0.71–0.713 pour les meilleurs modèles (SVM RBF et RandomForest), avec kNN et GradBoost juste derrière autour de 0.68–0.69.\n",
    "- Pour les 3 familles coarse, les F1‑scores restent raisonnables pour tous les modèles :\n",
    "\n",
    "    LogReg ≈ 0.56–0.65,\n",
    "\n",
    "    kNN ≈ 0.63–0.70,\n",
    "\n",
    "    SVM / RandomForest montent à 0.69–0.72, donc ils séparent mieux les genres.\n",
    "    ​\n",
    "\n",
    "- DecisionTree est le moins bon : accuracy ≈ 0.60 et F1 < 0.62 sur toutes les classes, ce qui montre qu’un seul arbre sur‑ajuste un peu le train mais généralise moins bien.\n",
    "\n",
    "- La macro‑avg et la weighted‑avg sont très proches pour tous les modèles (différence < 0.01–0.02), ce qui signifie que les trois classes sont apprises de façon assez équilibrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e862706e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MLP (validation) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nada\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.692     0.624     0.656      2524\n",
      "           1      0.730     0.688     0.708      3031\n",
      "           2      0.683     0.770     0.724      3372\n",
      "\n",
      "    accuracy                          0.700      8927\n",
      "   macro avg      0.702     0.694     0.696      8927\n",
      "weighted avg      0.702     0.700     0.699      8927\n",
      "\n",
      "\n",
      "=== AdaBoost (validation) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.657     0.471     0.548      2524\n",
      "           1      0.627     0.627     0.627      3031\n",
      "           2      0.582     0.706     0.638      3372\n",
      "\n",
      "    accuracy                          0.613      8927\n",
      "   macro avg      0.622     0.601     0.604      8927\n",
      "weighted avg      0.618     0.613     0.609      8927\n",
      "\n",
      "\n",
      "=== XGBoost (validation) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.771     0.717     0.743      2524\n",
      "           1      0.779     0.762     0.771      3031\n",
      "           2      0.747     0.800     0.773      3372\n",
      "\n",
      "    accuracy                          0.764      8927\n",
      "   macro avg      0.766     0.760     0.762      8927\n",
      "weighted avg      0.765     0.764     0.764      8927\n",
      "\n",
      "\n",
      "=== LightGBM (validation) ===\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9906\n",
      "[LightGBM] [Info] Number of data points in the train set: 26779, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.263029\n",
      "[LightGBM] [Info] Start training from score -1.080443\n",
      "[LightGBM] [Info] Start training from score -0.973500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.775     0.719     0.746      2524\n",
      "           1      0.775     0.764     0.769      3031\n",
      "           2      0.748     0.798     0.772      3372\n",
      "\n",
      "    accuracy                          0.764      8927\n",
      "   macro avg      0.766     0.760     0.762      8927\n",
      "weighted avg      0.765     0.764     0.764      8927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_boost = {\n",
    "    # Réseau de neurones (besoin de scaling)\n",
    "    \"MLP\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", MLPClassifier(\n",
    "            hidden_layer_sizes=(64, 32),\n",
    "            activation=\"relu\",\n",
    "            max_iter=200,\n",
    "            random_state=42\n",
    "        )),\n",
    "    ]),\n",
    "\n",
    "    # Boosting classique\n",
    "    \"AdaBoost\": AdaBoostClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.5,\n",
    "        random_state=42,\n",
    "    ),\n",
    "\n",
    "    # Gradient boosting optimisés\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_train)),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"multiclass\",\n",
    "        num_class=len(np.unique(y_train)),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "}\n",
    "\n",
    "for name, model in models_boost.items():\n",
    "    print(f\"\\n=== {name} (validation) ===\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    print(classification_report(y_val, y_val_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62435f0e",
   "metadata": {},
   "source": [
    "- MLP a une accuracy d’environ 0.70, avec des F1 par classe comprises entre 0.65 et 0.72; c’est un progrès\n",
    "- AdaBoost reste en dessous, avec une accuracy ~0.61 et un F1 plus faible pour la classe 0 (0.55), ce qui confirme que, sur ce dataset, il est moins adapté que les autres.\n",
    "- XGBoost et LightGBM donnent les meilleurs résultats, avec une accuracy de 0.764 et des F1 très proches pour les trois classes (≈ 0.74–0.78), d’où une macro‑F1 et une weighted‑F1 quasi identiques (~0.76), donc les trois familles de genres sont bien apprises de manière équilibrée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28799323",
   "metadata": {},
   "source": [
    "### 5 Sélection des meilleurs modèles et évaluation finale\n",
    "\n",
    "On sélectionne les 2 modèles les plus prometteurs (RandomForest, XGBoost et LightGBM).\n",
    "On les réentraîne sur `Train + Val` pour utiliser un maximum de données, puis on mesure une seule fois la performance sur le set de test tenu de côté.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc642b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest (test final) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.718     0.655     0.685      2524\n",
      "           1      0.725     0.723     0.724      3031\n",
      "           2      0.717     0.765     0.740      3372\n",
      "\n",
      "    accuracy                          0.720      8927\n",
      "   macro avg      0.720     0.715     0.716      8927\n",
      "weighted avg      0.720     0.720     0.719      8927\n",
      "\n",
      "\n",
      "=== XGBoost (test final) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.774     0.708     0.739      2524\n",
      "           1      0.781     0.775     0.778      3031\n",
      "           2      0.761     0.815     0.787      3372\n",
      "\n",
      "    accuracy                          0.771      8927\n",
      "   macro avg      0.772     0.766     0.768      8927\n",
      "weighted avg      0.771     0.771     0.770      8927\n",
      "\n",
      "\n",
      "=== LightGBM (test final) ===\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9921\n",
      "[LightGBM] [Info] Number of data points in the train set: 35706, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.263080\n",
      "[LightGBM] [Info] Start training from score -1.080379\n",
      "[LightGBM] [Info] Start training from score -0.973518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.777     0.702     0.738      2524\n",
      "           1      0.773     0.773     0.773      3031\n",
      "           2      0.758     0.813     0.784      3372\n",
      "\n",
      "    accuracy                          0.768      8927\n",
      "   macro avg      0.769     0.763     0.765      8927\n",
      "weighted avg      0.768     0.768     0.767      8927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_models = {\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_train)),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"multiclass\",\n",
    "        num_class=len(np.unique(y_train)),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "}\n",
    "\n",
    "X_train_full = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_full = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "# Évaluation finale sur le test\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\n=== {name} (test final) ===\")\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_test_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb669ffe",
   "metadata": {},
   "source": [
    "- RandomForest obtient une accuracy de 0.72, avec des F1 entre 0.69 et 0.74 sur les trois familles\n",
    "- XGBoost et LightGBM ont  autour de 0.77 d’accuracy sur le test, avec des F1 ≈ 0.74–0.79 pour chaque classe -> mieux que RandomForest\n",
    "- les macro‑moyennes et weighted‑moyennes sont quasi identiques pour les trois modèles (≈ 0.77 pour XGBoost/LightGBM) -> performances homogènes entre genres coarse.​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875765f",
   "metadata": {},
   "source": [
    "## Comparaison avec genre_top\n",
    "\n",
    "Sur Task 1 (genre_top, 16 classes), même les meilleurs modèles (XGBoost/LightGBM) montent à 0.74–0.77 d’accuracy, mais la F1 macro reste limitée (0.58–0.65) et la balanced accuracy plafonne vers 0.59, ce qui montre que certaines classes rares sont encore mal prédites.\n",
    "\n",
    "Sur Task 2 (3 genres coarse), XGBoost/LightGBM gardent une accuracy similaire (~0.77) mais la macro‑F1 grimpe à ~0.76 et la perf est très homogène entre classes (macro ≈ weighted), donc le modèle ne sacrifie plus les classes minoritaires."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "mon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
